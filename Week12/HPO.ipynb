{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a28f763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from nltk.stem.snowball import SnowballStemmer # Reduces words to their base form\n",
    "from sklearn.model_selection import train_test_split # Splits the data into train and test set\n",
    "import time\n",
    "\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "264cbeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into DFs\n",
    "df_train = pd.read_csv(r'C:\\Users\\vladp\\OneDrive\\Desktop\\Y3\\DataScience\\assignment3\\Datascience2025_A3\\Data\\train.csv', encoding=\"ISO-8859-1\")\n",
    "df_test = pd.read_csv(r'C:\\Users\\vladp\\OneDrive\\Desktop\\Y3\\DataScience\\assignment3\\Datascience2025_A3\\Data\\test.csv', encoding=\"ISO-8859-1\")\n",
    "df_pro_desc = pd.read_csv(r'C:\\Users\\vladp\\OneDrive\\Desktop\\Y3\\DataScience\\assignment3\\Datascience2025_A3\\Data\\product_descriptions.csv')\n",
    "\n",
    "num_train = df_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b72c4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function stems a string to normalize words\n",
    "def str_stemmer(s):\n",
    "\treturn \" \".join([stemmer.stem(word) for word in s.lower().split()])\n",
    "\n",
    "# This function counts how many words str1 and str2 have in common\n",
    "def str_common_word(str1, str2):\n",
    "\treturn sum(int(str2.find(word) >= 0) for word in str1.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fa9f225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines train and test dataframes to a signle one\n",
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "df_all = pd.merge(df_all, df_pro_desc, how='left', on='product_uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ddc59df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the stemmer to all text fields\n",
    "df_all['search_term'] = df_all['search_term'].map(lambda x:str_stemmer(x))\n",
    "df_all['product_title'] = df_all['product_title'].map(lambda x:str_stemmer(x))\n",
    "df_all['product_description'] = df_all['product_description'].map(lambda x:str_stemmer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aad58681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determines the num of words in search terms and concats all texts into a single string\n",
    "df_all['len_of_query'] = df_all['search_term'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "df_all['product_info'] = df_all['search_term'] + \"\\t\" + df_all['product_title'] + \"\\t\" + df_all['product_description']\n",
    "\n",
    "# Count how often titles and descriptions contain search words\n",
    "df_all['word_in_title'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0], x.split('\\t')[1]))\n",
    "df_all['word_in_description'] = df_all['product_info'].map(lambda x:str_common_word(x.split('\\t')[0], x.split('\\t')[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d613cb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits combined DF back into train and test set, keeping only the numerical values\n",
    "df_all = df_all.drop(['search_term','product_title','product_description','product_info'],axis=1)\n",
    "df_train = df_all.iloc[:num_train]\n",
    "df_test = df_all.iloc[num_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29a4c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts input features and labels\n",
    "id_test = df_test['id']\n",
    "y = df_train['relevance'].values\n",
    "X = df_train.drop(['id','relevance'], axis=1).values\n",
    "\n",
    "# Split the training set for evaluation (80% train, 20% validation)\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6be6513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Defining the parameter grid for optimization\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'min_samples_split': [2, 5, 10]  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c321d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply RandomizedSearchCV to find best params\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=gbr,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    "    n_jobs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fb21095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "HPO Processing Time: 3215.09 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "random_search.fit(X_train_split, y_train_split)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"HPO Processing Time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "deaa2d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'subsample': 0.9, 'n_estimators': 200, 'min_samples_split': 10, 'max_depth': 3, 'learning_rate': 0.05}\n",
      "Optimized GBR RMSE: 0.4808\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "# Make predictions on the validation split\n",
    "y_pred_optimized = random_search.predict(X_val_split)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_optimized = np.sqrt(mean_squared_error(y_val_split, y_pred_optimized))\n",
    "print(f\"Optimized GBR RMSE: {rmse_optimized:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
